"""
Phase 5: Offline RL Training (ASAP Dataset) - With Monitoring
=============================================================
åŠŸèƒ½å¢å¼ºï¼š
1. è®°å½•æ¯è½®çš„ Reward å’Œ Lossã€‚
2. å®æ—¶ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ° CSVã€‚
3. è®­ç»ƒç»“æŸåè‡ªåŠ¨ç»˜åˆ¶ Loss/Reward æ›²çº¿å›¾ã€‚
"""
import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from langchain_core.runnables import RunnableConfig

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from workflow.graph import mas_graph
from workflow.dqn_node import global_dqn_agent
from core.loaders.asap_loader import ASAPLoader

# ==========================================
# è·¯å¾„é…ç½®
# ==========================================
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data", "raw_submissions", "training_set_rel3.tsv")
METADATA_PATH = os.path.join(BASE_DIR, "data", "metadata", "asap_context.json")
LOG_DIR = os.path.join(BASE_DIR, "logs")  # æ–°å¢æ—¥å¿—ç›®å½•

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs(LOG_DIR, exist_ok=True)


def calculate_reward(final_state, ground_truth_score):
    """å¥–åŠ±å‡½æ•°"""
    reviews = final_state.get("reviews", [])
    if not reviews:
        return -1.0, 0.0

    last_reviews = reviews[-3:]
    if not last_reviews:
        return 0.0, 0.0

    avg_score = np.mean([r.overall_score for r in last_reviews])
    diff = abs(avg_score - ground_truth_score)

    accuracy_reward = 1.0 - diff
    rounds = final_state.get("current_round", 1)
    efficiency_penalty = 0.05 * (rounds - 1)

    total_reward = accuracy_reward - efficiency_penalty

    print(f"   ğŸ¯ Truth: {ground_truth_score:.2f} | Agents: {avg_score:.2f} | Diff: {diff:.2f}")
    print(f"   ğŸ’° Reward: {total_reward:.4f}")

    return total_reward, avg_score


def plot_metrics(metrics_df, save_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.figure(figsize=(12, 5))

    # 1. Reward æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(metrics_df['episode'], metrics_df['reward'], label='Reward', color='blue', alpha=0.6)
    # ç»˜åˆ¶ç§»åŠ¨å¹³å‡çº¿ (Window=5)
    if len(metrics_df) >= 5:
        plt.plot(metrics_df['episode'], metrics_df['reward'].rolling(5).mean(), label='Avg Reward (5)', color='red',
                 linewidth=2)
    plt.title("Training Reward over Episodes")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(True)

    # 2. Loss æ›²çº¿
    plt.subplot(1, 2, 2)
    # è¿‡æ»¤æ‰ None çš„ Loss
    loss_data = metrics_df[metrics_df['loss'].notna()]
    if not loss_data.empty:
        plt.plot(loss_data['episode'], loss_data['loss'], label='Loss', color='orange')
        plt.title("DQN Training Loss")
        plt.xlabel("Episode")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)
    else:
        plt.text(0.5, 0.5, 'No Loss Data Yet', ha='center')

    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ“Š ç›‘æ§å›¾è¡¨å·²ä¿å­˜: {save_path}")
    plt.close()


def train(episodes=50):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f">>> ğŸš€ å¯åŠ¨ ASAP è®­ç»ƒ (Episodes: {episodes}) | Session: {timestamp}")

    try:
        loader = ASAPLoader(tsv_path=DATA_PATH, metadata_path=METADATA_PATH)
        loader.load_dataset()
    except FileNotFoundError as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    train_indices = loader.get_split_indices('train')
    global_dqn_agent.policy_net.train()

    # ğŸ“Š ç›‘æ§æ•°æ®å®¹å™¨
    metrics_log = []

    for i in range(episodes):
        print(f"\nğŸ¬ Episode {i + 1}/{episodes}")
        rand_idx = np.random.choice(train_indices)
        subject, gt_score = loader.get_subject_by_index(rand_idx)

        print(f"   ğŸ“ ID: {subject.subject_id} (Set {subject.metadata['set_id']})")

        state = {"submission": subject, "reviews": [], "current_round": 1}

        episode_reward = 0.0
        episode_loss = None
        agent_score = 0.0

        try:
            final_state = mas_graph.invoke(
                state,
                config=RunnableConfig(run_name=f"Train_Ep_{i}")
            )

            # è®¡ç®—å¥–åŠ±
            episode_reward, agent_score = calculate_reward(final_state, gt_score)

            # æ¢¯åº¦æ›´æ–°
            loss = global_dqn_agent.update_policy(batch_size=16)
            if loss is not None:
                episode_loss = loss
                print(f"   ğŸ”¥ Loss: {loss:.4f}")

        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()

        # ğŸ“ è®°å½•æœ¬è½®æ•°æ®
        metrics_log.append({
            "episode": i + 1,
            "subject_id": subject.subject_id,
            "set_id": subject.metadata['set_id'],
            "ground_truth": gt_score,
            "agent_score": agent_score,
            "reward": episode_reward,
            "loss": episode_loss,
            "rounds": state.get("current_round", 1)
        })

        # æ¯ 10 è½®ä¿å­˜ä¸€æ¬¡ CSVï¼Œé˜²æ­¢ä¸­æ–­ä¸¢å¤±
        if (i + 1) % 10 == 0:
            df = pd.DataFrame(metrics_log)
            csv_path = os.path.join(LOG_DIR, f"training_log_{timestamp}.csv")
            df.to_csv(csv_path, index=False)
            print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {csv_path}")

    # ğŸ è®­ç»ƒç»“æŸ
    # 1. ä¿å­˜æ¨¡å‹
    model_path = os.path.join(BASE_DIR, "core", "dqn_weights_asap.pth")
    global_dqn_agent.save(model_path)
    print(f"\nğŸ’¾ æ¨¡å‹æƒé‡å·²ä¿å­˜: {model_path}")

    # 2. ä¿å­˜æœ€ç»ˆæ—¥å¿—
    df = pd.DataFrame(metrics_log)
    csv_path = os.path.join(LOG_DIR, f"training_log_{timestamp}.csv")
    df.to_csv(csv_path, index=False)

    # 3. ç»˜åˆ¶æ›²çº¿
    plot_path = os.path.join(LOG_DIR, f"training_curve_{timestamp}.png")
    plot_metrics(df, plot_path)


if __name__ == "__main__":
    # å»ºè®®è‡³å°‘è·‘ 50 è½®ä»¥è§‚å¯Ÿæ›²çº¿å˜åŒ–
    train(episodes=50)