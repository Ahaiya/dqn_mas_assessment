"""
Phase 5: Offline RL Training (ASAP Dataset) - Refactored
========================================================
é€‚é…æ–°çš„ç›®å½•ç»“æ„ï¼šåˆ†ç¦»æ•°æ®ä¸ä»£ç ã€‚
"""
import sys
import os
import numpy as np
from langchain_core.runnables import RunnableConfig

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from workflow.graph import mas_graph
from workflow.dqn_node import global_dqn_agent
# ğŸŒŸ è·¯å¾„å˜æ›´: ä» core.loaders å¯¼å…¥
from core.loaders.asap_loader import ASAPLoader

# ==========================================
# è·¯å¾„é…ç½®
# ==========================================
BASE_DIR = os.path.dirname(os.path.dirname(__file__))

# 1. æ•°æ®é›†è·¯å¾„ (TSV)
DATA_PATH = os.path.join(BASE_DIR, "data", "raw_submissions", "training_set_rel3.tsv")

# 2. å…ƒæ•°æ®è·¯å¾„ (JSON)
METADATA_PATH = os.path.join(BASE_DIR, "data", "metadata", "asap_context.json")


def calculate_reward(final_state, ground_truth_score):
    """å¥–åŠ±å‡½æ•° (ä¿æŒä¸å˜)"""
    reviews = final_state.get("reviews", [])
    if not reviews:
        return -1.0

    last_reviews = reviews[-3:]
    if not last_reviews:
        return 0.0

    avg_score = np.mean([r.overall_score for r in last_reviews])
    diff = abs(avg_score - ground_truth_score)

    accuracy_reward = 1.0 - diff
    rounds = final_state.get("current_round", 1)
    efficiency_penalty = 0.05 * (rounds - 1)

    total_reward = accuracy_reward - efficiency_penalty

    print(f"   ğŸ¯ Truth: {ground_truth_score:.2f} | Agents: {avg_score:.2f} | Diff: {diff:.2f}")
    print(f"   ğŸ’° Reward: {total_reward:.4f}")

    return total_reward


def train(episodes=10):
    print(f">>> ğŸš€ å¯åŠ¨ ASAP è®­ç»ƒ (Episodes: {episodes})")
    print(f"    TSV: {os.path.basename(DATA_PATH)}")
    print(f"    JSON: {os.path.basename(METADATA_PATH)}")

    # ğŸŒŸ åˆå§‹åŒ– Loader (ä¼ å…¥ä¸¤ä¸ªè·¯å¾„)
    try:
        loader = ASAPLoader(tsv_path=DATA_PATH, metadata_path=METADATA_PATH)
        loader.load_dataset()
    except FileNotFoundError as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # è·å–è®­ç»ƒé›†ç´¢å¼•
    train_indices = loader.get_split_indices('train')
    print(f"    è®­ç»ƒé›†å¤§å°: {len(train_indices)}")

    global_dqn_agent.policy_net.train()

    for i in range(episodes):
        print(f"\nğŸ¬ Episode {i + 1}/{episodes}")

        # éšæœºä»è®­ç»ƒé›†ä¸­é‡‡æ ·
        rand_idx = np.random.choice(train_indices)
        subject, gt_score = loader.get_subject_by_index(rand_idx)

        print(f"   ğŸ“ ID: {subject.subject_id} (Set {subject.metadata['set_id']})")

        state = {
            "submission": subject,
            "reviews": [],
            "current_round": 1
        }

        try:
            final_state = mas_graph.invoke(
                state,
                config=RunnableConfig(run_name=f"Train_Ep_{i}")
            )

            reward = calculate_reward(final_state, gt_score)

            # æ¨¡æ‹Ÿæ¢¯åº¦æ›´æ–°
            loss = global_dqn_agent.update_policy(batch_size=16)
            if loss:
                print(f"   ğŸ”¥ Loss: {loss:.4f}")

        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()

    global_dqn_agent.save(os.path.join(BASE_DIR, "core", "dqn_weights_asap.pth"))
    print("\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜ã€‚")


if __name__ == "__main__":
    train(episodes=5)