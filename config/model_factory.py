# è¿›ä¸€æ­¥å°è£… init_chat_model
# ç›®çš„æ˜¯: å·¥ç¨‹è§£è€¦ã€ç»Ÿä¸€ç®¡æ§
import os

from langchain.chat_models import init_chat_model
from functools import lru_cache
from dotenv import load_dotenv, find_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv(), override=True)

@lru_cache(maxsize=4) # ğŸŒŸ æœ€ä½³å®è·µ: ç¼“å­˜æ¨¡å‹å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–å¼€é”€
def get_core_model(temperature: float = 0.0):
    """
    [å•ä¾‹å·¥å‚] è¿”å›ç»Ÿä¸€é…ç½®çš„ Chat Modelã€‚
    åº•å±‚ä½¿ç”¨ LangChain 1.0 çš„ init_chat_modelã€‚
    """
    ## openai ç±»çš„ API æ¥å£
    # 1. é›†ä¸­è¯»å–é…ç½® (ä» .env)
    model_name = os.getenv("MODEL_NAME")
    base_url = os.getenv("OPENAI_API_BASE", None)

    # 2. ç»Ÿä¸€åˆå§‹åŒ–
    print(f"ğŸ­ ModelFactory: Loading {model_name} ...")

    if base_url:
        model = init_chat_model(
            model=model_name,
            temperature=temperature,
            openai_api_base=base_url
        )
    else:
        model = init_chat_model(
            model=model_name,
            temperature=temperature,
        )
    return model