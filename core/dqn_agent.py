"""
Layer 2: DQN Agent (å†³ç­–æ™ºèƒ½ä½“)
=========================================
åŠŸèƒ½: å°è£…ç¥ç»ç½‘ç»œï¼Œå®ç° Îµ-greedy å†³ç­–ã€ç»éªŒå›æ”¾ä¸æ¢¯åº¦æ›´æ–°ã€‚
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import random
import numpy as np
from collections import deque
from core.dqn_model import DQN

# ğŸŒŸ [æ–°å¢] å¼•å…¥ LangSmith è£…é¥°å™¨
from langsmith import traceable

class DQNAgent:
    def __init__(self, learning_rate=0.001, gamma=0.95, buffer_size=5000):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        :param learning_rate: å­¦ä¹ ç‡ (Alpha)
        :param gamma: æŠ˜æ‰£å› å­ (Gamma)ï¼Œå†³å®šçœ‹é‡çœ¼å‰åˆ©ç›Šè¿˜æ˜¯é•¿è¿œåˆ©ç›Š
        :param buffer_size: ç»éªŒå›æ”¾æ± å¤§å°
        """
        # 1. åˆå§‹åŒ–å¤§è„‘ (Policy Network)
        self.policy_net = DQN.default()
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)

        # 2. åˆå§‹åŒ–è®°å¿† (Replay Buffer)
        self.memory = deque(maxlen=buffer_size)

        # 3. è¶…å‚æ•°
        self.gamma = gamma
        self.action_space = [0, 1, 2]  # 0:ç»ˆæ­¢, 1:è¾©è®º, 2:æç¤º

    # ğŸŒŸ [å»ºè®®] å¢åŠ ç›‘æ§ï¼Œè¿™å¯¹äºåˆ†ææ¨¡å‹ä¸ºä»€ä¹ˆåšå†³å®šè‡³å…³é‡è¦
    @traceable(run_type="tool", name="DQN_Get_Q_Values")
    def get_q_values(self, state_tensor: torch.Tensor):
        """
        è·å–å½“å‰çŠ¶æ€ä¸‹çš„ Q å€¼ï¼Œç”¨äº LangSmith å¯è§†åŒ–ç›‘æ§
        """
        with torch.no_grad():
            # ç¡®ä¿ç»´åº¦åŒ¹é… [Batch, Dim] --- unsqueeze(0)æ’å…¥äº† batch ç»´åº¦
            if state_tensor.dim() == 1:
                state_tensor = state_tensor.unsqueeze(0)
            q_values = self.policy_net(state_tensor)
            # è½¬æˆæ™®é€š Python åˆ—è¡¨ [0.12, -0.5, 0.8]
            return q_values.squeeze().tolist()

    # ğŸŒŸ [ä¿®æ”¹] åŠ ä¸Š @traceable
    # run_type="tool" è¡¨ç¤ºè¿™åœ¨ LangSmith é‡Œä¼šè¢«æ˜¾ç¤ºä¸ºä¸€ä¸ªâ€œå·¥å…·è°ƒç”¨â€
    # name="DQN_Select" ç»™å®ƒèµ·ä¸ªæ˜“è¯»çš„åå­—
    @traceable(run_type="tool", name="DQN_Inference")
    def select_action(self, state_tensor: torch.Tensor, epsilon: float = 0.1) -> int:
        """
        æ ¸å¿ƒå†³ç­–é€»è¾‘ (Îµ-greedy ç­–ç•¥)
        :param state_tensor: çŠ¶æ€å‘é‡ (Layer 1 Output)
        :param epsilon: æ¢ç´¢ç‡ (0.0~1.0)ï¼Œè®­ç»ƒåˆæœŸé€šå¸¸è¾ƒé«˜ï¼ŒåæœŸé™ä½
        :return: åŠ¨ä½œç´¢å¼• (0, 1, 2)
        """
        # ç­–ç•¥ A: æ¢ç´¢ (Explore) - éšæœºçé€‰ï¼Œä¸ºäº†å‘ç°æ–°å¯èƒ½æ€§
        if random.random() < epsilon:
            return random.choice(self.action_space)

        # ç­–ç•¥ B: åˆ©ç”¨ (Exploit) - å¬å¤§è„‘çš„ï¼Œé€‰ Q å€¼æœ€å¤§çš„
        with torch.no_grad():
            # state_tensor ç»´åº¦å¯èƒ½æ˜¯ [6], éœ€è¦æ‰©å……ä¸º [1, 6] æ”¾å…¥ç½‘ç»œ
            if state_tensor.dim() == 1:
                state_tensor = state_tensor.unsqueeze(0)

            q_values = self.policy_net(state_tensor)
            # è¿”å› Q å€¼æœ€å¤§çš„åŠ¨ä½œç´¢å¼•
            return q_values.argmax().item()

    def store_transition(self, state, action, reward, next_state, done):
        """
        è®°å¿†å­˜å‚¨: å°†ä¸€æ®µç»å†å­˜å…¥å›æ”¾æ± 
        """
        self.memory.append((state, action, reward, next_state, done))

    # ğŸŒŸ [ä¿®æ”¹] åŠ ä¸Š @traceable ç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹
    @traceable(run_type="embedding", name="DQN_Training_Step")
    def update_policy(self, batch_size=32):
        """
        è‡ªæˆ‘è®­ç»ƒ: ä»è®°å¿†ä¸­éšæœºæŠ½å–ç‰‡æ®µï¼Œåå‘ä¼ æ’­æ›´æ–°å¤§è„‘
        (è¿™æ˜¯ Phase 5 è®­ç»ƒé˜¶æ®µçš„æ ¸å¿ƒï¼ŒPhase 2/3 æš‚æ—¶åªè°ƒç”¨æ¥å£)
        """
        if len(self.memory) < batch_size:
            return None # ç»éªŒå¤ªå°‘ï¼Œå…ˆä¸å­¦

        # 1. éšæœºæŠ½æ ·
        batch = random.sample(self.memory, batch_size)

        # 2. è§£åŒ…æ•°æ®
        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*batch)

        batch_state = torch.stack(batch_state)
        batch_action = torch.tensor(batch_action).unsqueeze(1)
        batch_reward = torch.tensor(batch_reward).unsqueeze(1)
        batch_next_state = torch.stack(batch_next_state)
        batch_done = torch.tensor(batch_done, dtype=torch.float32).unsqueeze(1)

        # 3. è®¡ç®—å½“å‰ Q å€¼ (Q_expected)
        # gather: æå–å‡ºå®é™…æ‰§è¡Œçš„é‚£ä¸ªåŠ¨ä½œå¯¹åº”çš„ Q å€¼
        q_values = self.policy_net(batch_state)
        current_q = q_values.gather(1, batch_action)

        # 4. è®¡ç®—ç›®æ ‡ Q å€¼ (Q_target) -> Bellman Equation
        with torch.no_grad():
            next_q_values = self.policy_net(batch_next_state)
            max_next_q = next_q_values.max(1)[0].unsqueeze(1)
            # Q_target = Reward + Gamma * Max(Next_Q) * (1 - Done)
            expected_q = batch_reward + (self.gamma * max_next_q * (1 - batch_done))

        # 5. è®¡ç®—æŸå¤± (MSE Loss)
        loss = F.mse_loss(current_q, expected_q)

        # 6. æ¢¯åº¦ä¸‹é™
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def save(self, path):
        torch.save(self.policy_net.state_dict(), path)

    def load(self, path):
        self.policy_net.load_state_dict(torch.load(path))