"""
Final Training Script (Configuration Driven)
============================================
"""
import os
import sys
import numpy as np
import pandas as pd
import math
import torch
from datetime import datetime
from langchain_core.runnables import RunnableConfig

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from workflow.graph import mas_graph
from workflow.dqn_node import global_dqn_agent
from core.loaders.asap_loader import ASAPLoader
from config.loader import global_config

# 1. è¯»å–é…ç½®
CONF = global_config["training"]
RUN_MODE = global_config.get("run_mode", "unknown")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data", "raw_submissions", "training_set_rel3.tsv")
METADATA_PATH = os.path.join(BASE_DIR, "data", "metadata", "asap_context.json")
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# å®šä¹‰å­˜æ¡£è·¯å¾„ (ç”¨äºæ–­ç‚¹ç»­è®­)
CHECKPOINT_PATH = os.path.join(BASE_DIR, "data", "model", "dqn_checkpoint.pth")

def get_epsilon(episode_idx):
    """è®¡ç®—å½“å‰è½®æ¬¡çš„æ¢ç´¢ç‡ (æŒ‡æ•°è¡°å‡)"""
    start = CONF["epsilon_start"]
    end = CONF["epsilon_end"]
    decay = CONF["epsilon_decay"]
    return end + (start - end) * math.exp(-1. * episode_idx / decay)


def calculate_reward(final_state, ground_truth_score):
    """
    å¥–åŠ±å‡½æ•°è®¾è®¡: Accuracy (å‡†ç¡®æ€§) - Efficiency (æ•ˆç‡)
    """
    reviews = final_state.get("reviews", [])
    if not reviews:
        return -1.0, 0.0

    # 1. è®¡ç®—é¢„æµ‹åˆ† (å–æœ€å3ä¸ªä¸“å®¶çš„å¹³å‡å€¼ï¼Œå‡è®¾æ˜¯3ä¸ªä¸“å®¶)
    num_agents = 3
    last_reviews = reviews[-num_agents:]
    pred_score = np.mean([r.overall_score for r in last_reviews])

    # 2. è®¡ç®—è¯¯å·® (0-5åˆ†åˆ¶)
    error = abs(pred_score - ground_truth_score)
    # 3. å‡†ç¡®æ€§å¥–åŠ± (æ»¡åˆ† 1.0)
    ## å¥–åŠ±: è¯¯å·®è¶Šå°è¶Šå¥½ã€‚æ»¡åˆ† 1.0ã€‚
    # ç­–ç•¥: error=0 -> 1.0; error=1 -> 0.6; error>=2.5 -> 0
    acc_reward = max(0, 1.0 - (error * 0.4))

    # 4. æ•ˆç‡æƒ©ç½š (æ¯å¤šä¸€è½®æ‰£ 0.05)
    rounds = final_state.get("current_round", 1)
    actual_rounds = max(1, rounds - 1)

    eff_penalty = 0.05 * (actual_rounds - 1)

    total = acc_reward - eff_penalty
    return total, pred_score

def save_checkpoint(episode, agent):
    """ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€"""
    state = {
        'episode': episode,
        'model_state_dict': agent.policy_net.state_dict(),
        'optimizer_state_dict': agent.optimizer.state_dict(),
        # å¦‚æœéœ€è¦ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥å­˜ target_netï¼Œä½†é€šå¸¸ load æ—¶é‡æ–° sync å³å¯
    }
    torch.save(state, CHECKPOINT_PATH)
    print(f" Checkpoint saved to {CHECKPOINT_PATH}")

def load_checkpoint(agent):
    """åŠ è½½æ–­ç‚¹"""
    if not os.path.exists(CHECKPOINT_PATH):
        print(" No checkpoint found, starting from scratch.")
        return 0

    try:
        checkpoint = torch.load(CHECKPOINT_PATH)
        agent.policy_net.load_state_dict(checkpoint['model_state_dict'])
        agent.target_net.load_state_dict(checkpoint['model_state_dict']) # åŒæ­¥ Target
        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_episode = checkpoint['episode'] + 1
        print(f" Resuming from Episode {start_episode}")
        return start_episode
    except Exception as e:
        print(f" Checkpoint load failed ({e}), starting from scratch.")
        return 0


def train():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"\n>>>  Starting DQN Training | Mode: {RUN_MODE} | Session: {timestamp}")
    print(f"    Params: Ep={CONF['total_episodes']}, Batch={CONF['batch_size']}, LR={CONF['learning_rate']}")

    # 1. åŠ è½½æ•°æ®
    loader = ASAPLoader(tsv_path=DATA_PATH, metadata_path=METADATA_PATH)
    try:
        loader.load_dataset()
    except Exception as e:
        print(f"âŒ Data Load Error: {e}")
        return

    train_indices = loader.get_split_indices('train')
    start_episode = load_checkpoint(global_dqn_agent)  # ğŸŒŸ åŠ è½½æ–­ç‚¹

    # 2. è®­ç»ƒå¾ªç¯
    global_dqn_agent.policy_net.train()
    metrics_log = []

    for i in range(start_episode, CONF["total_episodes"]):
        epsilon = get_epsilon(i)

        # A. éšæœºé‡‡æ ·ä¸€ä¸ªæ ·æœ¬ (Essay)
        idx = np.random.choice(train_indices)
        subject, gt_score = loader.get_subject_by_index(idx)

        # B. åˆå§‹åŒ–å›¾çŠ¶æ€
        state = {
            "submission": subject,
            "reviews": [],
            "current_round": 1,
            "epsilon": epsilon,
            "dqn_trace": [],    # è½¨è¿¹å®¹å™¨
            "dqn_action": -1
        }

        try:
            # C. è¿è¡Œ Graph
            final_state = mas_graph.invoke(state, config=RunnableConfig(run_name=f"Ep_{i}"))

            # D. ç»“ç®—å¥–åŠ±
            reward, pred_score = calculate_reward(final_state, gt_score)

            # E. å­˜å‚¨ç»éªŒ (Hindsight Experience Replay)
            trace = final_state.get("dqn_trace", [])

            if trace:
                for t, (s, a) in enumerate(trace):
                    is_last = (t == len(trace) - 1)
                    # Next State:
                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ­¥ï¼Œnext_s å°±æ˜¯ trace[t+1][0] (å³ä¸‹ä¸€è½®çš„çŠ¶æ€)
                    # å¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œnext_s æ— æ„ä¹‰ (å› ä¸º done=True)ï¼Œå¡«å½“å‰ s ä¿æŒæ ¼å¼
                    next_s = trace[t + 1][0] if not is_last else s

                    # Reward: ç¨€ç–å¥–åŠ±ï¼Œåªåœ¨æœ€åä¸€æ­¥ç»™
                    step_r = reward if is_last else 0.0

                    # å­˜å…¥ Buffer
                    global_dqn_agent.store_transition(s, a, step_r, next_s, is_last)

            # æ›´æ–°ç½‘ç»œ (ä»…åœ¨ Buffer è¶³å¤Ÿä¸”è¿‡é¢„çƒ­æœŸå)
            loss = None
            # if i > CONF["warmup_steps"]:
            #     loss = global_dqn_agent.update_policy(batch_size=CONF["batch_size"])
            loss = global_dqn_agent.update_policy(batch_size=CONF["batch_size"])

            # æ‰“å°æ—¥å¿— (æ¯ 10 è½®)
            if (i + 1) % 10 == 0:
                print(f"Ep {i + 1:04d} | Eps: {epsilon:.2f} | Rds: {final_state['current_round']} | "
                      f"GT: {gt_score:.1f} vs Pred: {pred_score:.1f} | Rw: {reward:.3f} | Loss: {loss}")
                # ğŸŒŸ å®šæœŸä¿å­˜ Checkpoint
                save_checkpoint(i, global_dqn_agent)

            metrics_log.append({
                "episode": i,
                "reward": reward,
                "loss": loss,
                "rounds": final_state['current_round'] - 1,
                "epsilon": epsilon,
                "gt": gt_score,
                "pred": pred_score
            })

        except Exception as e:
            print(f"âŒ Ep {i} Runtime Error: {e}")
            import traceback
            traceback.print_exc()

    # 3. ç»“æŸä¿å­˜
    log_path = os.path.join(LOG_DIR, f"train_log_{timestamp}.csv")
    pd.DataFrame(metrics_log).to_csv(log_path, index=False)

    model_path = os.path.join(BASE_DIR, "data", "model", "dqn_weights_final.pth")
    global_dqn_agent.save(model_path)

    print(f"\n Training Finished. Log: {log_path}, Model: {model_path}")


if __name__ == "__main__":
    train()